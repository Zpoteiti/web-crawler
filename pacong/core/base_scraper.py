"""
åŸºç¡€çˆ¬è™«æŠ½è±¡ç±»
å®šä¹‰æ‰€æœ‰çˆ¬è™«çš„é€šç”¨æ¥å£å’Œè¡Œä¸º
"""

import requests
import time
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from pathlib import Path

from .config import get_config
from .logger import get_logger, log_execution_time
from .exceptions import ScrapingError, ConfigurationError


class BaseScraper(ABC):
    """åŸºç¡€çˆ¬è™«æŠ½è±¡ç±»"""
    
    def __init__(self, name: str, **kwargs):
        self.name = name
        self.config = get_config()
        self.logger = get_logger(f"scraper.{name}")
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path(self.config.output.reports_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # çˆ¬è™«é…ç½®
        self.request_timeout = kwargs.get('timeout', self.config.scraping.request_timeout)
        self.retry_attempts = kwargs.get('retry_attempts', self.config.scraping.retry_attempts)
        self.rate_limit_delay = kwargs.get('rate_limit_delay', self.config.scraping.rate_limit_delay)
        
        # çŠ¶æ€è·Ÿè¸ª
        self._start_time: Optional[datetime] = None
        self._end_time: Optional[datetime] = None
        self._scraped_count = 0
        self._error_count = 0
        
        self.logger.info(f"ğŸš€ åˆå§‹åŒ–çˆ¬è™«: {self.name}")
    
    @abstractmethod
    def get_data_sources(self) -> List[Dict[str, str]]:
        """
        è·å–æ•°æ®æºåˆ—è¡¨
        è¿”å›æ ¼å¼: [{"name": "æ•°æ®æºåç§°", "url": "URL", "type": "æ•°æ®ç±»å‹"}]
        """
        pass
    
    @abstractmethod
    def scrape_single_source(self, source: Dict[str, str]) -> List[Dict[str, Any]]:
        """
        çˆ¬å–å•ä¸ªæ•°æ®æº
        
        Args:
            source: æ•°æ®æºä¿¡æ¯
            
        Returns:
            List[Dict]: çˆ¬å–åˆ°çš„æ•°æ®åˆ—è¡¨
        """
        pass
    
    @abstractmethod
    def validate_data(self, data: Dict[str, Any]) -> bool:
        """
        éªŒè¯å•æ¡æ•°æ®çš„æœ‰æ•ˆæ€§
        
        Args:
            data: å¾…éªŒè¯çš„æ•°æ®
            
        Returns:
            bool: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        """
        pass
    
    def clean_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ¸…æ´—å•æ¡æ•°æ®ï¼ˆå¯é‡å†™ï¼‰
        
        Args:
            data: åŸå§‹æ•°æ®
            
        Returns:
            Dict: æ¸…æ´—åçš„æ•°æ®
        """
        # é»˜è®¤å®ç°ï¼šæ·»åŠ æ—¶é—´æˆ³å’Œæ•°æ®æº
        cleaned_data = data.copy()
        cleaned_data.setdefault('timestamp', datetime.now())
        cleaned_data.setdefault('source', self.name)
        return cleaned_data
    
    @log_execution_time
    def scrape_all(self) -> List[Dict[str, Any]]:
        """
        çˆ¬å–æ‰€æœ‰æ•°æ®æº
        
        Returns:
            List[Dict]: æ‰€æœ‰çˆ¬å–åˆ°çš„æ•°æ®
        """
        self._start_time = datetime.now()
        self._scraped_count = 0
        self._error_count = 0
        
        self.logger.info(f"ğŸ“Š å¼€å§‹çˆ¬å–æ•°æ®æº: {self.name}")
        
        all_data = []
        data_sources = self.get_data_sources()
        
        self.logger.info(f"ğŸ“‹ å‘ç° {len(data_sources)} ä¸ªæ•°æ®æº")
        
        for i, source in enumerate(data_sources, 1):
            try:
                self.logger.info(f"ğŸ” [{i}/{len(data_sources)}] çˆ¬å–: {source.get('name', source.get('url'))}")
                
                # é€Ÿç‡é™åˆ¶
                if i > 1:
                    import time
                    time.sleep(self.rate_limit_delay)
                
                source_data = self.scrape_single_source(source)
                
                if source_data:
                    # æ•°æ®éªŒè¯å’Œæ¸…æ´—
                    valid_data = []
                    for item in source_data:
                        if self.validate_data(item):
                            cleaned_item = self.clean_data(item)
                            valid_data.append(cleaned_item)
                        else:
                            self.logger.warning(f"âš ï¸ æ•°æ®éªŒè¯å¤±è´¥: {item}")
                    
                    all_data.extend(valid_data)
                    self._scraped_count += len(valid_data)
                    
                    self.logger.info(f"âœ… è·å– {len(valid_data)} æ¡æœ‰æ•ˆæ•°æ®")
                else:
                    self.logger.warning(f"âš ï¸ æ•°æ®æºæ— æ•°æ®: {source.get('name')}")
                    
            except Exception as e:
                self._error_count += 1
                self.logger.error(f"âŒ çˆ¬å–å¤±è´¥: {source.get('name')} - {e}")
                continue
        
        self._end_time = datetime.now()
        execution_time = (self._end_time - self._start_time).total_seconds()
        
        self.logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆ: æˆåŠŸ {self._scraped_count} æ¡, é”™è¯¯ {self._error_count} æ¬¡, è€—æ—¶ {execution_time:.2f}ç§’")
        
        return all_data
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'scraper_name': self.name,
            'start_time': self._start_time,
            'end_time': self._end_time,
            'execution_time': (self._end_time - self._start_time).total_seconds() if self._start_time and self._end_time else None,
            'scraped_count': self._scraped_count,
            'error_count': self._error_count,
            'success_rate': self._scraped_count / (self._scraped_count + self._error_count) if (self._scraped_count + self._error_count) > 0 else 0
        }
    
    def save_raw_data(self, data: List[Dict[str, Any]], filename: Optional[str] = None) -> Path:
        """ä¿å­˜åŸå§‹æ•°æ®"""
        import json
        
        if not filename:
            timestamp = datetime.now().strftime(self.config.output.timestamp_format)
            filename = f"{self.name}_raw_{timestamp}.json"
        
        filepath = self.output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜: {filepath}")
        return filepath
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ”¯æŒ"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¸…ç†"""
        if hasattr(self, 'cleanup'):
            self.cleanup()


class WebScrapingMixin:
    """ç½‘é¡µçˆ¬è™«æ··å…¥ç±»"""
    
    def setup_http_session(self):
        """è®¾ç½®HTTPä¼šè¯"""
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        self.session = requests.Session()
        
        # é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=self.retry_attempts,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # é»˜è®¤å¤´éƒ¨
        self.session.headers.update({
            'User-Agent': self.config.browser.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
    
    def get_page(self, url: str, **kwargs) -> requests.Response:
        """è·å–ç½‘é¡µå†…å®¹"""
        if not hasattr(self, 'session'):
            self.setup_http_session()
        
        try:
            response = self.session.get(url, timeout=self.request_timeout, **kwargs)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            raise ScrapingError(f"è¯·æ±‚å¤±è´¥: {url}", url=url) from e
    
    def cleanup(self):
        """æ¸…ç†HTTPä¼šè¯"""
        if hasattr(self, 'session'):
            self.session.close()


class BrowserScrapingMixin:
    """æµè§ˆå™¨æ§åˆ¶çˆ¬è™«æ··å…¥ç±»"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.browser = None
        self.browser_type = kwargs.get('browser_type', 'selenium')
    
    def setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨é©±åŠ¨"""
        if self.browser_type == 'selenium':
            self._setup_selenium()
        elif self.browser_type == 'applescript':
            self._setup_applescript()
        elif self.browser_type == 'cdp':
            self._setup_cdp()
        else:
            raise ConfigurationError(f"ä¸æ”¯æŒçš„æµè§ˆå™¨ç±»å‹: {self.browser_type}")
    
    def _setup_selenium(self):
        """è®¾ç½®Seleniumé©±åŠ¨"""
        try:
            import undetected_chromedriver as uc
            from selenium.webdriver.chrome.options import Options
            
            options = Options()
            if self.config.get('browser.headless', True):
                options.add_argument("--headless")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument(f"--user-agent={self.config.browser.user_agent}")
            
            self.browser = uc.Chrome(options=options)
            self.browser.set_page_load_timeout(self.config.browser.selenium_timeout)
            
        except ImportError:
            raise ConfigurationError("éœ€è¦å®‰è£… undetected-chromedriver: pip install undetected-chromedriver")
        except Exception as e:
            raise ConfigurationError(f"Seleniumåˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _setup_applescript(self):
        """è®¾ç½®AppleScriptæ§åˆ¶"""
        # AppleScriptä¸éœ€è¦ç‰¹æ®Šåˆå§‹åŒ–
        self.browser = 'applescript'
    
    def _setup_cdp(self):
        """è®¾ç½®Chrome DevTools Protocol"""
        # CDPè¿æ¥ä¼šåœ¨ä½¿ç”¨æ—¶å»ºç«‹
        self.browser = 'cdp'
    
    def get_page_content(self, url: str) -> str:
        """è·å–é¡µé¢å†…å®¹"""
        if not self.browser:
            self.setup_browser()
        
        if self.browser_type == 'selenium':
            return self._get_content_selenium(url)
        elif self.browser_type == 'applescript':
            return self._get_content_applescript(url)
        elif self.browser_type == 'cdp':
            return self._get_content_cdp(url)
    
    def _get_content_selenium(self, url: str) -> str:
        """é€šè¿‡Seleniumè·å–å†…å®¹"""
        self.browser.get(url)
        return self.browser.page_source
    
    def _get_content_applescript(self, url: str) -> str:
        """é€šè¿‡AppleScriptè·å–å†…å®¹"""
        from ..browser.applescript import execute_applescript
        
        script = f'''
        tell application "Google Chrome"
            set URL of active tab of front window to "{url}"
            delay 5
            execute active tab of front window javascript "document.documentElement.outerHTML"
        end tell
        '''
        
        return execute_applescript(script)
    
    def _get_content_cdp(self, url: str) -> str:
        """é€šè¿‡CDPè·å–å†…å®¹"""
        # CDPå®ç°ä¼šæ›´å¤æ‚ï¼Œè¿™é‡Œæ˜¯åŸºæœ¬æ¡†æ¶
        raise NotImplementedError("CDPæ”¯æŒå¾…å®ç°")
    
    def cleanup(self):
        """æ¸…ç†æµè§ˆå™¨èµ„æº"""
        if self.browser and self.browser_type == 'selenium':
            try:
                self.browser.quit()
            except:
                pass 