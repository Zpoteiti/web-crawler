#!/usr/bin/env python3
"""
æ··åˆå¼å•†å“æ•°æ®è·å–ç³»ç»Ÿ
ç»“åˆAPIè°ƒç”¨å’Œç½‘é¡µæŠ“å–ï¼Œæä¾›æœ€å…¨é¢çš„æ•°æ®è¦†ç›–
"""

import requests
import json
import re
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from pathlib import Path
import logging

import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.common.exceptions import WebDriverException, TimeoutException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class WebScrapingDataSource:
    """
    ç½‘é¡µæŠ“å–æ•°æ®æº
    ä½¿ç”¨Seleniumé©±åŠ¨çš„æ— å¤´æµè§ˆå™¨è¿›è¡Œæ•°æ®æŠ“å–
    """
    
    def __init__(self):
        try:
            options = uc.ChromeOptions()
            options.add_argument("--headless")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
            
            self.driver = uc.Chrome(options=options, use_subprocess=True)
            self.driver.set_page_load_timeout(60) # å»¶é•¿é¡µé¢åŠ è½½è¶…æ—¶åˆ°60ç§’
        except WebDriverException as e:
            logger.error(f"åˆå§‹åŒ–WebDriverå¤±è´¥: {e}")
            logger.error("è¯·ç¡®ä¿æ‚¨çš„ç³»ç»Ÿå·²å®‰è£…Chromeæµè§ˆå™¨ã€‚")
            self.driver = None

    def _get_page_source(self, url: str, wait_for_element: tuple = None) -> str:
        """è·å–é¡µé¢æºä»£ç ï¼Œå¹¶å¯é€‰æ‹©ç­‰å¾…ç‰¹å®šå…ƒç´ åŠ è½½å®Œæˆ"""
        if not self.driver:
            return ""
        try:
            self.driver.get(url)
            if wait_for_element:
                WebDriverWait(self.driver, 20).until(
                    EC.presence_of_element_located(wait_for_element)
                )
            else:
                time.sleep(5) 
            return self.driver.page_source
        except TimeoutException:
            logger.error(f"ç­‰å¾…å…ƒç´  {wait_for_element} è¶…æ—¶ã€‚é¡µé¢: {url}")
            return self.driver.page_source # å³ä½¿è¶…æ—¶ï¼Œä¹Ÿå°è¯•è¿”å›å·²åŠ è½½å†…å®¹
        except Exception as e:
            logger.error(f"ä½¿ç”¨SeleniumåŠ è½½é¡µé¢ {url} å¤±è´¥: {e}")
            return ""

    def scrape_investing_com(self) -> list:
        """ä»Investing.comè·å–å•†å“ä»·æ ¼"""
        url = "https://www.investing.com/commodities/"
        logger.info(f"æ­£åœ¨ä½¿ç”¨SeleniumæŠ“å–: {url}")
        
        # ç­‰å¾…æ•°æ®è¡¨æ ¼çš„å®¹å™¨å‡ºç°
        html_content = self._get_page_source(url, wait_for_element=(By.CSS_SELECTOR, "table[data-test='commodities-table']"))
        if not html_content:
            return []

        soup = BeautifulSoup(html_content, 'html.parser')
        commodities = []
        
        table = soup.find('table', attrs={'data-test': 'commodities-table'})
        if not table:
            logger.warning("åœ¨Investing.comä¸Šæœªæ‰¾åˆ°å•†å“æ•°æ®è¡¨ã€‚")
            return []

        rows = table.find_all('tr', attrs={'data-test': re.compile(r'row-')})
        for row in rows:
            try:
                name_cell = row.find('td', attrs={'data-test': 'cell-name'})
                price_cell = row.find('td', attrs={'data-test': 'cell-last'})
                change_cell = row.find('td', attrs={'data-test': 'cell-change_percentage'})

                if name_cell and price_cell and change_cell:
                    name = name_cell.get_text(strip=True)
                    price = float(price_cell.get_text(strip=True).replace(',', ''))
                    change_pct_text = change_cell.get_text(strip=True).replace('%', '')
                    change = float(change_pct_text)
                    
                    commodities.append({
                        'name': name,
                        'price': price,
                        'change': change,
                        'source': 'investing.com',
                        'timestamp': datetime.now()
                    })
            except (ValueError, AttributeError) as e:
                continue

        logger.info(f"ä»Investing.comè·å–äº† {len(commodities)} ä¸ªå•†å“ä»·æ ¼")
        return commodities
    
    def scrape_yahoo_finance_commodities(self) -> list:
        """ä»Yahoo Financeè·å–å•†å“æ•°æ®"""
        commodities = []
        yahoo_symbols = {
            'GC=F': 'é»„é‡‘æœŸè´§', 'SI=F': 'ç™½é“¶æœŸè´§', 'CL=F': 'WTIåŸæ²¹',
            'BZ=F': 'å¸ƒä¼¦ç‰¹åŸæ²¹', 'NG=F': 'å¤©ç„¶æ°”', 'HG=F': 'é“œæœŸè´§'
        }
        
        for symbol, name in yahoo_symbols.items():
            url = f"https://finance.yahoo.com/quote/{symbol}"
            logger.info(f"æ­£åœ¨ä½¿ç”¨SeleniumæŠ“å–: {url}")
            # ç­‰å¾…ä»·æ ¼å…ƒç´ å‡ºç°
            html_content = self._get_page_source(url, wait_for_element=(By.CSS_SELECTOR, f"fin-streamer[data-symbol='{symbol}']"))
            if not html_content:
                continue

            soup = BeautifulSoup(html_content, 'html.parser')
            
            try:
                # ä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
                price_elem = soup.find('fin-streamer', {'data-symbol': symbol, 'data-field': 'regularMarketPrice'})
                change_elem = soup.find('fin-streamer', {'data-symbol': symbol, 'data-field': 'regularMarketChangePercent'})

                if price_elem and change_elem:
                    price = float(price_elem.get('value'))
                    change = float(change_elem.get('value'))
                    
                    commodities.append({
                        'name': name,
                        'symbol': symbol,
                        'price': price,
                        'change': change,
                        'source': 'yahoo.com',
                        'timestamp': datetime.now()
                    })
            except (ValueError, AttributeError, TypeError) as e:
                logger.warning(f"è§£æ {symbol} æ•°æ®å¤±è´¥: {e}")
                continue

        logger.info(f"ä»Yahoo Financeè·å–äº† {len(commodities)} ä¸ªå•†å“ä»·æ ¼")
        return commodities

    def scrape_bloomberg_commodities(self) -> list:
        """ä»Bloomberg.comè·å–å•†å“æ•°æ®"""
        url = "https://www.bloomberg.com/markets/commodities"
        logger.info(f"æ­£åœ¨ä½¿ç”¨SeleniumæŠ“å–: {url}")
        # ç­‰å¾…è¡¨æ ¼å®¹å™¨å‡ºç°
        html_content = self._get_page_source(url, wait_for_element=(By.XPATH, '//div[contains(@class, "table-container")]'))
        if not html_content:
            return []

        soup = BeautifulSoup(html_content, 'html.parser')
        commodities = []

        # Bloombergçš„é¡µé¢ç»“æ„éå¸¸å¤æ‚ï¼ŒXPathæ˜¯æ›´å¥½çš„é€‰æ‹©
        # æ³¨æ„ï¼šè¿™ä¸ªXPathæ˜¯ç¤ºä¾‹ï¼Œå¯èƒ½éšæ—¶å¤±æ•ˆ
        try:
            # ç›´æ¥ä»å·²ç»åŠ è½½çš„soupå¯¹è±¡ä¸­æŸ¥æ‰¾ï¼Œè€Œä¸æ˜¯å†æ¬¡é©±åŠ¨driver
            table_container = soup.find('div', class_=re.compile(r"table-container"))
            if not table_container:
                logger.warning("åœ¨Bloombergé¡µé¢ä¸Šæœªæ‰¾åˆ°table-containerã€‚")
                return []
                
            rows = table_container.find_all('div', class_=re.compile(r"data-table-row"))
            for row in rows:
                cells = row.find_all('div', class_=re.compile(r"data-table-cell"))
                if len(cells) > 4:
                    name = cells[0].get_text(strip=True)
                    price_text = cells[1].get_text(strip=True)
                    change_text = cells[3].get_text(strip=True)

                    price = float(price_text.replace(',', ''))
                    change = float(change_text.replace('%', ''))
                    
                    if name and price:
                        commodities.append({
                            'name': name,
                            'price': price,
                            'change': change,
                            'source': 'bloomberg.com',
                            'timestamp': datetime.now()
                        })
        except Exception as e:
            logger.error(f"åœ¨Bloombergé¡µé¢ä¸Šè§£ææ•°æ®å¤±è´¥: {e}")

        logger.info(f"ä»Bloombergè·å–äº† {len(commodities)} ä¸ªå•†å“ä»·æ ¼")
        return commodities

    def close(self):
        """å…³é—­WebDriver"""
        if self.driver:
            self.driver.quit()
            logger.info("WebDriverå·²å…³é—­ã€‚")


class HybridCommoditySystem:
    """
    æ··åˆå¼å•†å“æ•°æ®ç³»ç»Ÿ
    é€šè¿‡ç½‘é¡µæŠ“å–ï¼Œæä¾›å…¨é¢çš„æ•°æ®
    """
    
    def __init__(self):
        self.web_scraper = WebScrapingDataSource()
        self.output_dir = Path("reports")
        self.output_dir.mkdir(exist_ok=True)
    
    def collect_all_data(self) -> dict:
        """æ”¶é›†æ‰€æœ‰æ•°æ®æºçš„å•†å“ä¿¡æ¯"""
        logger.info("ğŸ” å¼€å§‹æ”¶é›†å¤šæºå•†å“æ•°æ®...")
        
        all_data = {
            'web_data': [],
            'combined_data': [],
            'collection_time': datetime.now()
        }
        
        # ä»ç½‘é¡µæŠ“å–æ•°æ®
        web_sources = [
            self.web_scraper.scrape_yahoo_finance_commodities,
            self.web_scraper.scrape_investing_com,
            self.web_scraper.scrape_bloomberg_commodities,
        ]
        
        for scraper_func in web_sources:
            try:
                scraped_data = scraper_func()
                all_data['web_data'].extend(scraped_data)
            except Exception as e:
                logger.error(f"ç½‘é¡µæŠ“å–å¤±è´¥: {e}")
        
        # åˆå¹¶å’Œå»é‡æ•°æ®
        all_data['combined_data'] = self._merge_commodity_data(all_data['web_data'])
        
        logger.info(f"âœ… æ•°æ®æ”¶é›†å®Œæˆ: ç½‘é¡µ={len(all_data['web_data'])}, åˆå¹¶å={len(all_data['combined_data'])}")
        
        return all_data
    
    def _merge_commodity_data(self, commodity_list: list) -> list:
        """åˆå¹¶å’Œå»é‡å•†å“æ•°æ®"""
        commodity_dict = {}
        
        for commodity in commodity_list:
            name = commodity['name'].lower()
            
            # æ ‡å‡†åŒ–å•†å“åç§°
            name_mapping = {
                'é»„é‡‘': 'gold',
                'é»„é‡‘æœŸè´§': 'gold',
                'gold futures': 'gold',
                'ç™½é“¶': 'silver',
                'ç™½é“¶æœŸè´§': 'silver',
                'silver futures': 'silver',
                'åŸæ²¹': 'crude_oil',
                'wtiåŸæ²¹': 'crude_oil',
                'crude oil': 'crude_oil',
                'å¤©ç„¶æ°”': 'natural_gas',
                'natural gas': 'natural_gas',
                'å°éº¦': 'wheat',
                'å°éº¦æœŸè´§': 'wheat',
                'wheat futures': 'wheat',
                'ç‰ç±³': 'corn',
                'ç‰ç±³æœŸè´§': 'corn',
                'corn futures': 'corn',
                'é“œ': 'copper',
                'é“œæœŸè´§': 'copper',
                'copper futures': 'copper',
            }
            
            standard_name = name_mapping.get(name, name)
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæŠ“å–åˆ°çš„æ•°æ®æº
            if standard_name not in commodity_dict:
                commodity_dict[standard_name] = commodity
        
        return list(commodity_dict.values())
    
    def generate_comprehensive_report(self, data: dict) -> str:
        """ç”Ÿæˆç»¼åˆæ•°æ®æŠ¥å‘Š"""
        report = []
        report.append("ğŸ“Š æ··åˆå¼å•†å“æ•°æ®ç»¼åˆæŠ¥å‘Š")
        report.append(f"ğŸ• ç”Ÿæˆæ—¶é—´: {data['collection_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("=" * 60)
        
        # æ•°æ®æºç»Ÿè®¡
        report.append(f"\nğŸ“ˆ æ•°æ®æºç»Ÿè®¡:")
        report.append(f"   ç½‘é¡µæŠ“å–: {len(data['web_data'])} ä¸ªåŸå§‹æ¡ç›®")
        report.append(f"   åˆå¹¶å: {len(data['combined_data'])} ä¸ªç‹¬ç«‹å•†å“")
        
        # ä¸»è¦å•†å“ä»·æ ¼
        report.append(f"\nğŸ’° ä¸»è¦å•†å“ä»·æ ¼:")
        report.append("-" * 50)
        report.append(f"{'å•†å“':15} {'ä»·æ ¼':>12} {'å˜åŒ–':>10} {'æ•°æ®æº':>15}")
        report.append("-" * 50)
        
        for commodity in sorted(data['combined_data'], key=lambda x: x['name']):
            name = commodity['name']
            price = commodity['price']
            change = commodity.get('change', 0)
            source = commodity['source']
            
            change_str = f"{change:+.2f}" if change != 0 else "N/A"
            trend = "â†—ï¸" if change > 0 else "â†˜ï¸" if change < 0 else "â¡ï¸"
            
            report.append(f"{name:15} ${price:>11.2f} {change_str:>9} {source:>15} {trend}")
        
        # æ•°æ®è´¨é‡åˆ†æ
        web_count = len(data['web_data'])
        
        if web_count > 0:
            report.append(f"\nğŸ“Š æ•°æ®è´¨é‡åˆ†æ:")
            report.append(f"   æ•°æ®æ¥æº: Yahoo Finance, Investing.com, Bloomberg.com")
            report.append(f"   æ•°æ®æ–°é²œåº¦: å®æ—¶ (æŠ“å–æ—¶)")
        
        return "\n".join(report)
    
    def create_comparison_chart(self, data: dict):
        """åˆ›å»ºæ•°æ®æºå¯¹æ¯”å›¾è¡¨"""
        if not data['combined_data']:
            return None
        
        # å‡†å¤‡æ•°æ®
        commodities = data['combined_data']
        names = [c['name'] for c in commodities]
        prices = [c['price'] for c in commodities]
        sources = [c['source'] for c in commodities]
        changes = [c.get('change', 0) for c in commodities]
        
        # åˆ›å»ºå›¾è¡¨
        fig = make_subplots(
            rows=2, cols=1,
            subplot_titles=['å•†å“å½“å‰ä»·æ ¼', 'ä»·æ ¼å˜åŒ–'],
            vertical_spacing=0.1
        )
        
        # ä»·æ ¼æŸ±çŠ¶å›¾
        fig.add_trace(
            go.Bar(
                x=names,
                y=prices,
                name='å½“å‰ä»·æ ¼',
                marker_color='orange',
                text=[f"${p:.2f}" for p in prices],
                textposition='outside'
            ),
            row=1, col=1
        )
        
        # å˜åŒ–æ•£ç‚¹å›¾
        change_colors = ['green' if c > 0 else 'red' if c < 0 else 'gray' for c in changes]
        
        fig.add_trace(
            go.Scatter(
                x=names,
                y=changes,
                mode='markers+text',
                name='ä»·æ ¼å˜åŒ–',
                marker=dict(
                    size=10,
                    color=change_colors
                ),
                text=[f"{c:+.2f}" for c in changes],
                textposition='top center'
            ),
            row=2, col=1
        )
        
        fig.update_layout(
            title="æ··åˆå¼å•†å“æ•°æ®å¯¹æ¯”",
            height=800,
            template='plotly_white'
        )
        
        # ä¿å­˜å›¾è¡¨
        chart_file = self.output_dir / f"hybrid_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        fig.write_html(chart_file)
        logger.info(f"å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {chart_file}")
        
        return chart_file
    
    def save_data_to_csv(self, data: dict):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not data['combined_data']:
            return None
        
        df = pd.DataFrame(data['combined_data'])
        csv_file = self.output_dir / f"hybrid_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°CSV: {csv_file}")
        
        return csv_file
    
    def run_comprehensive_analysis(self):
        """è¿è¡Œç»¼åˆåˆ†æ"""
        logger.info("ğŸš€ å¯åŠ¨æ··åˆå¼å•†å“æ•°æ®åˆ†æç³»ç»Ÿ")
        
        # æ”¶é›†æ•°æ®
        data = self.collect_all_data()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_comprehensive_report(data)
        print(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / f"hybrid_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # åˆ›å»ºå›¾è¡¨
        self.create_comparison_chart(data)
        
        # ä¿å­˜CSV
        self.save_data_to_csv(data)
        
        logger.info(f"âœ… ç»¼åˆåˆ†æå®Œæˆï¼Œæ–‡ä»¶ä¿å­˜åœ¨: {self.output_dir}")
        
        # å…³é—­æµè§ˆå™¨é©±åŠ¨
        self.web_scraper.close()

        return data

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ç½‘é¡µæŠ“å–å•†å“æ•°æ®ç³»ç»Ÿ")
    print("=" * 50)
    print("é€šè¿‡æŠ“å–å¤šä¸ªç½‘ç«™ï¼Œæä¾›æœ€å…¨é¢çš„å•†å“æ•°æ®")
    print()
    
    # åˆ›å»ºç³»ç»Ÿ
    system = HybridCommoditySystem()
    
    print("\nğŸš€ å¼€å§‹æ”¶é›†å’Œåˆ†ææ•°æ®...")
    
    # è¿è¡Œåˆ†æ
    try:
        data = system.run_comprehensive_analysis()
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š å…±è·å– {len(data['combined_data'])} ä¸ªå•†å“çš„æ•°æ®")
        print(f"ğŸ“ æŠ¥å‘Šæ–‡ä»¶ä¿å­˜åœ¨: {system.output_dir}")
        
    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main() 